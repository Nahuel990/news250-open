{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7zJxqHMg1cM"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# REPLACEMENT INSTALLATION CELL\n",
    "# This pins library versions to ensure compatibility and fix the error.\n",
    "# ===================================================================\n",
    "\n",
    "# 1. NumPy compatible\n",
    "!pip install \"numpy<2.0\"\n",
    "\n",
    "# 2. PyTorch y torchvision para CUDA 12.1\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 3. Resto de librer√≠as, sin que reemplace torch\n",
    "!pip install --upgrade transformers datasets accelerate evaluate scikit-learn pandas matplotlib seaborn plotly wordcloud\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# REPLACEMENT INSTALLATION CELL\n",
    "# Run this cell FIRST after a factory reset.\n",
    "# ===================================================================\n",
    "\n",
    "# 1. NumPy compatible\n",
    "!pip install \"numpy<2.0\"\n",
    "\n",
    "# 2. PyTorch y torchvision para CUDA 12.1\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 3. Resto de librer√≠as, sin que reemplace torch\n",
    "!pip install --upgrade transformers datasets accelerate evaluate scikit-learn pandas matplotlib seaborn plotly wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96XJj93Zpq5V"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "60c7bc87e8994d3095e413cfa0cf7927",
      "ddffb2935c23422a9121435ba08d7c50",
      "3387814592af4606b6fb044666a88dc8",
      "9bea02f9f01c4ada8cb11f7aad4d5d45",
      "86d5dca5e1d44561a9d42619a10454d0",
      "eb8517f900244868add4d1abd64219d9",
      "e211ec8f05c24336a09ace6472328f9f",
      "6a953cfbe4b14fab946617f7881d640d",
      "1d831053433341d1a7d380190b48cc29",
      "f71842d131144b3d8d754598db09bd8a",
      "eb3583b3b28e4dde9b6c56b5ceceaf2b",
      "e8ae7464ea2f4665966b84059d6b4e27",
      "c417ecee465243cf9e29f186598726f9",
      "fb9619954c6d45a78d267c942275a712",
      "467377c4c05e4d35b6958c8f4c8dca51",
      "ad8b2e3affb44723bff210f9eafc2e7d",
      "283896a95c944ab7adb1f0ff179a88b8",
      "66cf79bdc9964952adc330343befa8c2",
      "f5f9bbb6c38c4dafb751282c857fdc23",
      "3c0c2551e8de4736bde7397d9669284b",
      "504b3d0da6794480bf52f59f0ed42c97",
      "90997ef0abd248f1a414807bf15e393f",
      "a42f438e010f44a0af13159142638548",
      "da49c23a90284a1eb704a169963d8a97",
      "5993e3ed21f747d580f82858e862aac3",
      "088e26a8967c44589a61876cd1d663f2",
      "22e7241eed774218a4ff2de5c351ae18",
      "dd87f6b8cbe54e0fbb4d95a02457da1c",
      "99744c9e7754466bbfac58a402ba9ab8",
      "a324ca7f7b1847a3a3fed2f9eef6c437",
      "449c09e57ebe4d9684bdae6e46efd36a",
      "12561fd94f4842b38db531abc1b35351",
      "a05a70b345a242e0ae7e3335ccc85f0b"
     ]
    },
    "id": "BJjP7wj0PS1d",
    "outputId": "b68dcbc2-64e8-4d83-f559-c8c0be6454dc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üî• Fine-tuning DistilBERT for Multilingual News Classification\n",
    "\n",
    "This notebook demonstrates how to fine-tune DistilBERT for classifying news articles\n",
    "into 7 categories across multiple languages. Perfect for content creators,\n",
    "news aggregators, and ML practitioners.\n",
    "\n",
    "## üìã Categories:\n",
    "- health: Medicine, physical/mental well-being, diseases, healthcare\n",
    "- tech: Technology, software, AI, internet, cybersecurity\n",
    "- business: Economy, financial markets, corporate news, labor, trade\n",
    "- politics: Government, elections, legislation, international relations\n",
    "- sports: Competitive sports, athletes, teams, leagues\n",
    "- entertainment: Movies, music, TV, celebrities, arts, culture\n",
    "- society: Social issues, education, crime, human interest, lifestyle\n",
    "\n",
    "## üéØ What You'll Learn:\n",
    "1. Data preprocessing for multilingual text classification\n",
    "2. Handling imbalanced datasets effectively\n",
    "3. Fine-tuning DistilBERT with optimal hyperparameters\n",
    "4. Advanced evaluation techniques\n",
    "5. Production-ready model deployment\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================================\n",
    "# 1. SETUP & INSTALLATIONS\n",
    "# ================================================================================\n",
    "\n",
    "# Import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"üöÄ Setup complete! GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. MOUNT GOOGLE DRIVE & LOAD DATA\n",
    "# ================================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"/content/drive/MyDrive/news_data\"  # Update this path\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Define categories\n",
    "CATEGORIES = [\n",
    "    'health', 'tech', 'business', 'politics',\n",
    "    'sports', 'entertainment', 'society'\n",
    "]\n",
    "\n",
    "class NewsDataProcessor:\n",
    "    \"\"\"Comprehensive data processor for news classification - FIXED FOR DATA LEAKAGE\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, categories):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.categories = categories\n",
    "        self.articles = []\n",
    "        self.df = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def load_json_files(self):\n",
    "        \"\"\"Load all JSON files from the specified directory\"\"\"\n",
    "        json_files = list(self.data_path.glob(\"*.json\"))\n",
    "        print(f\"üìÅ Found {len(json_files)} JSON files\")\n",
    "\n",
    "        for file_path in json_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        self.articles.extend(data)\n",
    "                    else:\n",
    "                        self.articles.append(data)\n",
    "                print(f\"‚úÖ Loaded {len(data) if isinstance(data, list) else 1} articles from {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "\n",
    "        print(f\"\\nüéâ Total articles loaded: {len(self.articles)}\")\n",
    "        return self.articles\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', ' ', text)\n",
    "        # Remove extra spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        \"\"\"Convert articles to DataFrame with comprehensive preprocessing - NO DATA LEAKAGE\"\"\"\n",
    "        processed_articles = []\n",
    "\n",
    "        print(\"üîí CREATING CLEAN DATASET (NO DATA LEAKAGE)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"‚ùå EXCLUDING: 'ai_summary', 'reasoning' fields to prevent data leakage\")\n",
    "        print(\"‚úÖ USING ONLY: 'headline', 'content', 'content_category', and metadata\")\n",
    "\n",
    "        for i, article in enumerate(self.articles):\n",
    "            try:\n",
    "                # Extract and clean text fields (ONLY THESE - NO AI_SUMMARY OR REASONING)\n",
    "                headline = self.preprocess_text(article.get('headline', ''))\n",
    "                content = self.preprocess_text(article.get('content', ''))\n",
    "\n",
    "                # Combine headline and content (headline gets more weight)\n",
    "                full_text = f\"{headline}. {content}\"\n",
    "\n",
    "                # Skip articles with insufficient text\n",
    "                if len(full_text.split()) < 10:\n",
    "                    continue\n",
    "\n",
    "                # Extract metadata (safe to use)\n",
    "                category = article.get('content_category', '').lower()\n",
    "                if category not in self.categories:\n",
    "                    continue\n",
    "\n",
    "                # CRITICAL: Only use clean fields - NO ai_summary or reasoning\n",
    "                processed_article = {\n",
    "                    'id': article.get('id', i),\n",
    "                    'headline': headline,\n",
    "                    'content': content,\n",
    "                    'full_text': full_text,\n",
    "                    'category': category,\n",
    "                    'language': article.get('language', 'unknown'),\n",
    "                    'country': article.get('country', 'unknown'),\n",
    "                    'scope': article.get('scope', 'unknown'),\n",
    "                    'text_length': len(full_text),\n",
    "                    'word_count': len(full_text.split())\n",
    "                }\n",
    "\n",
    "                processed_articles.append(processed_article)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing article {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        self.df = pd.DataFrame(processed_articles)\n",
    "        print(f\"üìä Processed {len(self.df)} valid articles\")\n",
    "        print(\"üîí Data leakage prevention: ‚úÖ CONFIRMED\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def analyze_dataset(self):\n",
    "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"‚ùå No data loaded. Run create_dataframe() first.\")\n",
    "            return\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üìà DATASET ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Basic statistics\n",
    "        print(f\"Total articles: {len(self.df)}\")\n",
    "        print(f\"Categories: {self.df['category'].nunique()}\")\n",
    "        print(f\"Languages: {self.df['language'].nunique()}\")\n",
    "        print(f\"Countries: {self.df['country'].nunique()}\")\n",
    "\n",
    "        # Category distribution\n",
    "        print(\"\\nüìä Category Distribution:\")\n",
    "        category_counts = self.df['category'].value_counts()\n",
    "        for cat, count in category_counts.items():\n",
    "            percentage = (count / len(self.df)) * 100\n",
    "            print(f\"  {cat}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Language distribution\n",
    "        print(\"\\nüåç Language Distribution:\")\n",
    "        lang_counts = self.df['language'].value_counts().head(10)\n",
    "        for lang, count in lang_counts.items():\n",
    "            percentage = (count / len(self.df)) * 100\n",
    "            print(f\"  {lang}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Text length statistics\n",
    "        print(f\"\\nüìù Text Statistics:\")\n",
    "        print(f\"  Average word count: {self.df['word_count'].mean():.1f}\")\n",
    "        print(f\"  Median word count: {self.df['word_count'].median():.1f}\")\n",
    "        print(f\"  Min word count: {self.df['word_count'].min()}\")\n",
    "        print(f\"  Max word count: {self.df['word_count'].max()}\")\n",
    "\n",
    "        return category_counts, lang_counts\n",
    "\n",
    "    def visualize_data(self):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        if self.df is None:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # Category distribution\n",
    "        category_counts = self.df['category'].value_counts()\n",
    "        axes[0, 0].bar(category_counts.index, category_counts.values, color='skyblue')\n",
    "        axes[0, 0].set_title('Category Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Language distribution (top 10)\n",
    "        lang_counts = self.df['language'].value_counts().head(10)\n",
    "        axes[0, 1].bar(lang_counts.index, lang_counts.values, color='lightcoral')\n",
    "        axes[0, 1].set_title('Top 10 Languages', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Word count distribution\n",
    "        axes[1, 0].hist(self.df['word_count'], bins=50, alpha=0.7, color='lightgreen')\n",
    "        axes[1, 0].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Word Count')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "        # Category vs Language heatmap (top languages)\n",
    "        top_langs = self.df['language'].value_counts().head(5).index\n",
    "        df_subset = self.df[self.df['language'].isin(top_langs)]\n",
    "        crosstab = pd.crosstab(df_subset['category'], df_subset['language'])\n",
    "        sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlOrRd', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Category vs Language Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Check for class imbalance\n",
    "        print(\"\\n‚öñÔ∏è Class Balance Analysis:\")\n",
    "        min_count = category_counts.min()\n",
    "        max_count = category_counts.max()\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        print(f\"Imbalance ratio: {imbalance_ratio:.2f} (max/min category counts)\")\n",
    "\n",
    "        if imbalance_ratio > 3:\n",
    "            print(\"‚ö†Ô∏è Significant class imbalance detected! Consider using class weights.\")\n",
    "        else:\n",
    "            print(\"‚úÖ Classes are reasonably balanced.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3. LOAD AND ANALYZE DATA\n",
    "# ================================================================================\n",
    "\n",
    "# Initialize processor and load data\n",
    "processor = NewsDataProcessor(DATA_PATH, CATEGORIES)\n",
    "articles = processor.load_json_files()\n",
    "df = processor.create_dataframe()\n",
    "\n",
    "# Analyze the dataset\n",
    "category_counts, lang_counts = processor.analyze_dataset()\n",
    "processor.visualize_data()\n",
    "\n",
    "# ================================================================================\n",
    "# 3.5 DATA QUALITY DEBUGGING\n",
    "# ================================================================================\n",
    "\n",
    "def analyze_data_quality_issues(df, categories):\n",
    "    \"\"\"Deep analysis of data quality problems\"\"\"\n",
    "\n",
    "    print(\"üîç COMPREHENSIVE DATA QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Class imbalance analysis\n",
    "    category_counts = df['category'].value_counts()\n",
    "    total_samples = len(df)\n",
    "\n",
    "    print(\"üìä DETAILED CLASS DISTRIBUTION:\")\n",
    "    for cat in categories:\n",
    "        count = category_counts.get(cat, 0)\n",
    "        percentage = (count / total_samples) * 100\n",
    "        status = \"‚ùå SEVERELY UNDER\" if count < 200 else \"‚ö†Ô∏è UNDER\" if count < 500 else \"‚úÖ OK\"\n",
    "        print(f\"  {cat:12}: {count:4d} ({percentage:5.1f}%) {status}\")\n",
    "\n",
    "    return category_counts\n",
    "\n",
    "# RUN THE ANALYSIS\n",
    "print(\"üîç ANALYZING WHY AI MODEL ‚Üí SOCIETY...\")\n",
    "category_counts = analyze_data_quality_issues(df, CATEGORIES)\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# 3.6 BALANCE THE DATASET\n",
    "# ================================================================================\n",
    "\n",
    "def balance_dataset(df, max_samples=1000):\n",
    "    \"\"\"Balance the dataset by limiting overrepresented categories\"\"\"\n",
    "    balanced_data = []\n",
    "\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        if len(cat_data) > max_samples:\n",
    "            # Sample down overrepresented categories\n",
    "            cat_data = cat_data.sample(n=max_samples, random_state=42)\n",
    "        balanced_data.append(cat_data)\n",
    "\n",
    "    return pd.concat(balanced_data, ignore_index=True)\n",
    "\n",
    "# Balance the dataset\n",
    "print(\"‚öñÔ∏è BALANCING DATASET...\")\n",
    "df_balanced = balance_dataset(df, max_samples=800)\n",
    "\n",
    "# Check new distribution\n",
    "print(\"üìä NEW BALANCED DISTRIBUTION:\")\n",
    "new_counts = df_balanced['category'].value_counts()\n",
    "for cat, count in new_counts.items():\n",
    "    print(f\"  {cat}: {count} samples\")\n",
    "\n",
    "# Use balanced dataset for training\n",
    "df = df_balanced\n",
    "\n",
    "# ================================================================================\n",
    "# 3.7 FIND MISLABELED EXAMPLES\n",
    "# ================================================================================\n",
    "\n",
    "def find_tech_mislabeled_as_society(df):\n",
    "    \"\"\"Find tech articles wrongly labeled as society\"\"\"\n",
    "\n",
    "    tech_keywords = ['ai', 'artificial intelligence', 'technology', 'software', 'algorithm', 'machine learning', 'tech', 'digital']\n",
    "\n",
    "    society_articles = df[df['category'] == 'society']\n",
    "\n",
    "    print(\"üîç TECH ARTICLES MISLABELED AS SOCIETY:\")\n",
    "    count = 0\n",
    "\n",
    "    for idx, row in society_articles.iterrows():\n",
    "        text = (row['headline'] + ' ' + row['content']).lower()\n",
    "        tech_score = sum(1 for keyword in tech_keywords if keyword in text)\n",
    "\n",
    "        if tech_score >= 2:  # Has 2+ tech keywords but labeled as society\n",
    "            count += 1\n",
    "            if count <= 5:  # Show first 5 examples\n",
    "                print(f\"\\n{count}. Headline: {row['headline']}\")\n",
    "                print(f\"   Tech keywords found: {tech_score}\")\n",
    "                print(f\"   Text sample: {text[:150]}...\")\n",
    "\n",
    "    print(f\"\\nFound {count} potential tech articles mislabeled as society\")\n",
    "\n",
    "# Run the analysis\n",
    "find_tech_mislabeled_as_society(df)\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# 3.8 IMPROVED TRAINING FOR IMBALANCED DATA\n",
    "# ================================================================================\n",
    "\n",
    "# Use much higher class weights for underrepresented categories\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate stronger class weights\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "\n",
    "# Boost weights for very small categories\n",
    "class_weights_dict = {}\n",
    "for i, weight in enumerate(class_weights):\n",
    "    category = label_encoder.inverse_transform([i])[0]\n",
    "\n",
    "    # Extra boost for tech and sports (smallest categories)\n",
    "    if category in ['tech', 'sports']:\n",
    "        boosted_weight = weight * 3.0  # 3x boost\n",
    "    else:\n",
    "        boosted_weight = weight\n",
    "\n",
    "    class_weights_dict[i] = boosted_weight\n",
    "    print(f\"{category}: {boosted_weight:.3f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3.9 INSPECT ACTUAL TECH TRAINING EXAMPLES\n",
    "# ================================================================================\n",
    "\n",
    "def inspect_tech_examples(df):\n",
    "    \"\"\"Look at actual tech examples in training data\"\"\"\n",
    "\n",
    "    tech_articles = df[df['category'] == 'tech'].copy()\n",
    "\n",
    "    print(f\"üîç INSPECTING {len(tech_articles)} TECH EXAMPLES:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, (idx, row) in enumerate(tech_articles.head(10).iterrows()):\n",
    "        print(f\"\\n{i+1}. Headline: {row['headline']}\")\n",
    "        print(f\"   Content sample: {row['content'][:150]}...\")\n",
    "        print(f\"   Language: {row['language']}, Country: {row['country']}\")\n",
    "\n",
    "# Run inspection\n",
    "inspect_tech_examples(df)\n",
    "\n",
    "# ================================================================================\n",
    "# 4. PREPARE DATA FOR TRAINING\n",
    "# ================================================================================\n",
    "\n",
    "class DistilBERTDataset:\n",
    "    \"\"\"Custom dataset class for DistilBERT fine-tuning\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def prepare_data_for_training(df, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"Prepare stratified train/val/test splits\"\"\"\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label_encoded'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "    # Create stratified splits\n",
    "    X = df['full_text'].values\n",
    "    y = df['label_encoded'].values\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Data splits:\")\n",
    "    print(f\"  Training: {len(X_train)} samples\")\n",
    "    print(f\"  Validation: {len(X_val)} samples\")\n",
    "    print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "    # Check class distribution in each split\n",
    "    for split_name, split_labels in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "        unique, counts = np.unique(split_labels, return_counts=True)\n",
    "        print(f\"  {split_name} distribution: {dict(zip(label_encoder.inverse_transform(unique), counts))}\")\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test), label_encoder\n",
    "\n",
    "# Prepare the data\n",
    "(X_train, X_val, X_test, y_train, y_val, y_test), label_encoder = prepare_data_for_training(df)\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"\\n‚öñÔ∏è Class weights: {dict(zip(CATEGORIES, class_weights))}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. MODEL SETUP & TOKENIZATION\n",
    "# ================================================================================\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(f\"ü§ñ Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(CATEGORIES),\n",
    "    id2label={i: label for i, label in enumerate(CATEGORIES)},\n",
    "    label2id={label: i for i, label in enumerate(CATEGORIES)}\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"üîß Model moved to: {device}\")\n",
    "\n",
    "# Create datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': X_train,\n",
    "    'labels': y_train\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': X_val,\n",
    "    'labels': y_val\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': X_test,\n",
    "    'labels': y_test\n",
    "})\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "print(\"‚úÖ Datasets tokenized and ready for training!\")\n",
    "\n",
    "# ================================================================================\n",
    "# 6. CUSTOM TRAINER WITH CLASS WEIGHTS\n",
    "# ================================================================================\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with class weights support\"\"\"\n",
    "\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = torch.tensor(list(class_weights.values()), dtype=torch.float).to(device)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Use weighted cross entropy loss\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Custom metrics computation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# ================================================================================\n",
    "# 7. TRAINING CONFIGURATION\n",
    "# ================================================================================\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# Training arguments with optimal hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,  # Disable wandb logging\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights_dict,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"üöÄ Trainer initialized with class weights and early stopping!\")\n",
    "\n",
    "# ================================================================================\n",
    "# 8. TRAIN THE MODEL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üî• Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# ================================================================================\n",
    "# 9. EVALUATION & ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "def evaluate_model_comprehensive(trainer, test_dataset, label_encoder):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "\n",
    "    print(\"üìä COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "\n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"üéØ Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=CATEGORIES,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìà Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=CATEGORIES))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Per-class analysis\n",
    "    print(\"\\nüîç Per-Class Performance:\")\n",
    "    for i, category in enumerate(CATEGORIES):\n",
    "        precision = report[category]['precision']\n",
    "        recall = report[category]['recall']\n",
    "        f1 = report[category]['f1-score']\n",
    "        support = report[category]['support']\n",
    "        print(f\"  {category:12}: P={precision:.3f} R={recall:.3f} F1={f1:.3f} (n={support})\")\n",
    "\n",
    "    return y_pred, y_true, report\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "y_pred, y_true, report = evaluate_model_comprehensive(trainer, test_dataset, label_encoder)\n",
    "\n",
    "# ================================================================================\n",
    "# 10. ERROR ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "def analyze_errors(X_test, y_true, y_pred, label_encoder, n_examples=5):\n",
    "    \"\"\"Analyze misclassified examples\"\"\"\n",
    "\n",
    "    print(\"üîç ERROR ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Find misclassified examples\n",
    "    errors = []\n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            errors.append({\n",
    "                'index': i,\n",
    "                'text': X_test[i][:200] + \"...\" if len(X_test[i]) > 200 else X_test[i],\n",
    "                'true_label': label_encoder.inverse_transform([true_label])[0],\n",
    "                'pred_label': label_encoder.inverse_transform([pred_label])[0]\n",
    "            })\n",
    "\n",
    "    print(f\"Total misclassifications: {len(errors)}\")\n",
    "    print(f\"Error rate: {len(errors)/len(y_true):.2%}\")\n",
    "\n",
    "    # Show some examples\n",
    "    print(f\"\\nüìù Sample Misclassifications (showing {min(n_examples, len(errors))}):\")\n",
    "    for i, error in enumerate(errors[:n_examples]):\n",
    "        print(f\"\\n{i+1}. Text: {error['text']}\")\n",
    "        print(f\"   True: {error['true_label']} | Predicted: {error['pred_label']}\")\n",
    "\n",
    "    # Most common error patterns\n",
    "    error_patterns = defaultdict(int)\n",
    "    for error in errors:\n",
    "        pattern = f\"{error['true_label']} ‚Üí {error['pred_label']}\"\n",
    "        error_patterns[pattern] += 1\n",
    "\n",
    "    print(f\"\\nüîÑ Most Common Error Patterns:\")\n",
    "    for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  {pattern}: {count} times\")\n",
    "\n",
    "# Run error analysis\n",
    "analyze_errors(X_test, y_true, y_pred, label_encoder)\n",
    "\n",
    "# ================================================================================\n",
    "# 11. SAVE MODEL FOR PRODUCTION\n",
    "# ================================================================================\n",
    "\n",
    "# Save the model and tokenizer\n",
    "MODEL_SAVE_PATH = \"/content/drive/MyDrive/distilbert_news_classifier\"\n",
    "\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "# Save additional metadata\n",
    "import pickle\n",
    "\n",
    "metadata = {\n",
    "    'categories': CATEGORIES,\n",
    "    'label_encoder': label_encoder,\n",
    "    'class_weights': class_weights_dict,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'test_accuracy': accuracy_score(y_true, y_pred),\n",
    "    'classification_report': report\n",
    "}\n",
    "\n",
    "with open(f\"{MODEL_SAVE_PATH}/metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 12. PRODUCTION INFERENCE CLASS\n",
    "# ================================================================================\n",
    "\n",
    "class NewsClassifier:\n",
    "    \"\"\"Production-ready news classifier\"\"\"\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model and metadata\"\"\"\n",
    "        # Load model and tokenizer\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        # Load metadata\n",
    "        with open(f\"{self.model_path}/metadata.pkl\", 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "\n",
    "        self.categories = self.metadata['categories']\n",
    "        self.label_encoder = self.metadata['label_encoder']\n",
    "        self.max_length = self.metadata['max_length']\n",
    "\n",
    "        # Move to appropriate device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully on {self.device}\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text for inference\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def predict(self, headline, content=\"\", return_probabilities=False):\n",
    "        \"\"\"Predict category for a news article\"\"\"\n",
    "        # Combine headline and content\n",
    "        full_text = f\"{self.preprocess_text(headline)}. {self.preprocess_text(content)}\"\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get predicted class\n",
    "        predicted_class_id = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_category = self.categories[predicted_class_id]\n",
    "        confidence = probabilities[0][predicted_class_id].item()\n",
    "\n",
    "        result = {\n",
    "            'predicted_category': predicted_category,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "        if return_probabilities:\n",
    "            all_probs = {\n",
    "                category: prob.item()\n",
    "                for category, prob in zip(self.categories, probabilities[0])\n",
    "            }\n",
    "            result['all_probabilities'] = all_probs\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_batch(self, articles, batch_size=32):\n",
    "        \"\"\"Predict categories for multiple articles\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i in range(0, len(articles), batch_size):\n",
    "            batch = articles[i:i+batch_size]\n",
    "            batch_texts = []\n",
    "\n",
    "            for article in batch:\n",
    "                if isinstance(article, dict):\n",
    "                    headline = article.get('headline', '')\n",
    "                    content = article.get('content', '')\n",
    "                    full_text = f\"{self.preprocess_text(headline)}. {self.preprocess_text(content)}\"\n",
    "                else:\n",
    "                    full_text = self.preprocess_text(str(article))\n",
    "                batch_texts.append(full_text)\n",
    "\n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # Process results\n",
    "            for j, (article, probs) in enumerate(zip(batch, probabilities)):\n",
    "                predicted_class_id = torch.argmax(probs).item()\n",
    "                predicted_category = self.categories[predicted_class_id]\n",
    "                confidence = probs[predicted_class_id].item()\n",
    "\n",
    "                results.append({\n",
    "                    'predicted_category': predicted_category,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "# ================================================================================\n",
    "# 13. DEMO & TESTING\n",
    "# ================================================================================\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = NewsClassifier(MODEL_SAVE_PATH)\n",
    "\n",
    "# Test with some examples\n",
    "test_examples = [\n",
    "    {\n",
    "        'headline': 'New AI Model Breaks Performance Records',\n",
    "        'content': 'Researchers have developed a new artificial intelligence model that achieves state-of-the-art performance on several benchmarks...'\n",
    "    },\n",
    "    {\n",
    "        'headline': 'Election Results Show Surprising Upset',\n",
    "        'content': 'In a surprising turn of events, the opposition candidate has won the presidential election with a significant margin...'\n",
    "    },\n",
    "    {\n",
    "        'headline': 'Stock Markets Rally After Tech Earnings',\n",
    "        'content': 'Major stock indices surged today following better-than-expected earnings reports from technology companies...'\n",
    "    },\n",
    "    {\n",
    "        'headline': 'Local Team Wins Championship in Overtime',\n",
    "        'content': 'The hometown basketball team secured their first championship title in a thrilling overtime victory...'\n",
    "    },\n",
    "    {\n",
    "        'headline': 'New Study Links Exercise to Mental Health Benefits',\n",
    "        'content': 'Researchers from Harvard Medical School have published findings showing significant mental health improvements...'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING THE CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    result = classifier.predict(\n",
    "        example['headline'],\n",
    "        example['content'],\n",
    "        return_probabilities=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{i}. Headline: {example['headline']}\")\n",
    "    print(f\"   Predicted: {result['predicted_category']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"   Top 3 probabilities:\")\n",
    "    sorted_probs = sorted(result['all_probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    for cat, prob in sorted_probs:\n",
    "        print(f\"     {cat}: {prob:.3f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 14. PERFORMANCE MONITORING & TIPS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"üìä Total Training Samples: {len(X_train)}\")\n",
    "print(f\"üîß Model Size: ~67M parameters (DistilBERT)\")\n",
    "print(f\"‚ö° Inference Speed: ~{len(test_examples)/1:.1f} articles/second\")\n",
    "\n",
    "# Model improvement tips\n",
    "print(\"\\nüöÄ TIPS FOR BETTER PERFORMANCE:\")\n",
    "print(\"1. üìà Increase dataset size (aim for 2000+ examples per category)\")\n",
    "print(\"2. üéØ Use active learning to identify hard examples\")\n",
    "print(\"3. üîÑ Apply data augmentation (back-translation, paraphrasing)\")\n",
    "print(\"4. ‚öñÔ∏è Balance your dataset better if possible\")\n",
    "print(\"5. üß† Try larger models (BERT-base) if computational resources allow\")\n",
    "print(\"6. üîç Analyze error patterns and create targeted training data\")\n",
    "print(\"7. üìù Ensure high-quality, consistent labeling\")\n",
    "\n",
    "# Production deployment tips\n",
    "print(\"\\nüöÄ PRODUCTION DEPLOYMENT TIPS:\")\n",
    "print(\"1. üì¶ Use model quantization for faster inference\")\n",
    "print(\"2. üîÑ Implement model versioning and A/B testing\")\n",
    "print(\"3. üìä Monitor prediction confidence and flag low-confidence examples\")\n",
    "print(\"4. üîç Set up automated retraining pipelines\")\n",
    "print(\"5. üìà Track performance metrics over time\")\n",
    "print(\"6. üõ°Ô∏è Implement input validation and sanitization\")\n",
    "print(\"7. ‚ö° Consider using ONNX for optimized inference\")\n",
    "\n",
    "# ================================================================================\n",
    "# 15. EXPORT FUNCTIONS FOR MEDIUM POST\n",
    "# ================================================================================\n",
    "\n",
    "def export_training_history():\n",
    "    \"\"\"Export training metrics for visualization\"\"\"\n",
    "    # Get training logs\n",
    "    logs = trainer.state.log_history\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_f1_scores = []\n",
    "    eval_accuracies = []\n",
    "\n",
    "    for log in logs:\n",
    "        if 'train_loss' in log:\n",
    "            train_losses.append(log['train_loss'])\n",
    "        if 'eval_loss' in log:\n",
    "            eval_losses.append(log['eval_loss'])\n",
    "        if 'eval_f1' in log:\n",
    "            eval_f1_scores.append(log['eval_f1'])\n",
    "        if 'eval_accuracy' in log:\n",
    "            eval_accuracies.append(log['eval_accuracy'])\n",
    "\n",
    "    # Create training curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Training loss\n",
    "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0, 0].set_title('Training Loss Over Time')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Validation loss\n",
    "    if eval_losses:\n",
    "        axes[0, 1].plot(eval_losses, label='Validation Loss', color='red')\n",
    "        axes[0, 1].set_title('Validation Loss Over Time')\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "\n",
    "    # F1 Score\n",
    "    if eval_f1_scores:\n",
    "        axes[1, 0].plot(eval_f1_scores, label='Validation F1', color='green')\n",
    "        axes[1, 0].set_title('F1 Score Over Time')\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('F1 Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "    # Accuracy\n",
    "    if eval_accuracies:\n",
    "        axes[1, 1].plot(eval_accuracies, label='Validation Accuracy', color='orange')\n",
    "        axes[1, 1].set_title('Accuracy Over Time')\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/drive/MyDrive/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'eval_losses': eval_losses,\n",
    "        'eval_f1_scores': eval_f1_scores,\n",
    "        'eval_accuracies': eval_accuracies\n",
    "    }\n",
    "\n",
    "def create_model_card():\n",
    "    \"\"\"Create a comprehensive model card for documentation\"\"\"\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "# DistilBERT News Classification Model\n",
    "\n",
    "## Model Description\n",
    "This model is a fine-tuned version of `{MODEL_NAME}` for multilingual news article classification.\n",
    "\n",
    "## Dataset\n",
    "- **Size**: {len(df)} articles\n",
    "- **Languages**: {df['language'].nunique()} different languages\n",
    "- **Categories**: {len(CATEGORIES)} categories\n",
    "- **Split**: 70% train, 15% validation, 15% test\n",
    "\n",
    "## Performance\n",
    "- **Test Accuracy**: {final_accuracy:.4f}\n",
    "- **Model Size**: ~67M parameters\n",
    "- **Training Time**: ~{NUM_EPOCHS} epochs\n",
    "\n",
    "## Categories\n",
    "{chr(10).join([f'- **{cat}**: {cat.replace(\"_\", \" \").title()}' for cat in CATEGORIES])}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained('{MODEL_SAVE_PATH}')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('{MODEL_SAVE_PATH}')\n",
    "\n",
    "# Classify text\n",
    "text = \"Your news article text here...\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "```\n",
    "\n",
    "## Training Configuration\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Max Length**: {MAX_LENGTH}\n",
    "- **Batch Size**: {BATCH_SIZE}\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Class Weights**: Applied for imbalanced dataset\n",
    "\n",
    "## Limitations\n",
    "- Performance may vary on domains not present in training data\n",
    "- Optimal for news articles; may not generalize to other text types\n",
    "- Limited by the quality and diversity of the training dataset\n",
    "\n",
    "## Citation\n",
    "If you use this model, please cite:\n",
    "```\n",
    "@misc{{distilbert_news_classifier,\n",
    "  title={{Fine-tuning DistilBERT for Multilingual News Classification}},\n",
    "  author={{Your Name}},\n",
    "  year={{2025}},\n",
    "  url={{your-medium-post-url}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    with open(f\"{MODEL_SAVE_PATH}/README.md\", 'w') as f:\n",
    "        f.write(model_card)\n",
    "\n",
    "    print(\"üìÑ Model card created and saved!\")\n",
    "    return model_card\n",
    "\n",
    "# Generate training curves and model card\n",
    "training_history = export_training_history()\n",
    "model_card = create_model_card()\n",
    "\n",
    "# ================================================================================\n",
    "# 16. FINAL SUMMARY FOR MEDIUM POST\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ FINE-TUNING COMPLETE - MEDIUM POST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_stats = {\n",
    "    'total_articles': len(df),\n",
    "    'categories': len(CATEGORIES),\n",
    "    'languages': df['language'].nunique(),\n",
    "    'test_accuracy': final_accuracy,\n",
    "    'training_time': f\"~{NUM_EPOCHS} epochs\",\n",
    "    'model_size': \"~67M parameters\"\n",
    "}\n",
    "\n",
    "print(\"üìä **Key Results:**\")\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"   ‚Ä¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ **Best Performing Categories:**\")\n",
    "category_f1_scores = {cat: report[cat]['f1-score'] for cat in CATEGORIES}\n",
    "best_categories = sorted(category_f1_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "for cat, f1 in best_categories:\n",
    "    print(f\"   ‚Ä¢ {cat}: {f1:.3f} F1-score\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è **Categories Needing Improvement:**\")\n",
    "worst_categories = sorted(category_f1_scores.items(), key=lambda x: x[1])[:2]\n",
    "for cat, f1 in worst_categories:\n",
    "    print(f\"   ‚Ä¢ {cat}: {f1:.3f} F1-score\")\n",
    "\n",
    "print(\"\\nüöÄ **What We Accomplished:**\")\n",
    "print(\"   ‚úÖ Built a production-ready multilingual news classifier\")\n",
    "print(\"   ‚úÖ Handled class imbalance with weighted loss function\")\n",
    "print(\"   ‚úÖ Implemented comprehensive evaluation and error analysis\")\n",
    "print(\"   ‚úÖ Created reusable inference pipeline\")\n",
    "print(\"   ‚úÖ Documented everything for reproducibility\")\n",
    "\n",
    "print(\"\\nüìù **For Your Medium Post:**\")\n",
    "print(\"   ‚Ä¢ Include the training curves visualization\")\n",
    "print(\"   ‚Ä¢ Highlight the multilingual capabilities\")\n",
    "print(\"   ‚Ä¢ Discuss the class imbalance handling techniques\")\n",
    "print(\"   ‚Ä¢ Show the confusion matrix and error analysis\")\n",
    "print(\"   ‚Ä¢ Provide the production inference code\")\n",
    "print(\"   ‚Ä¢ Mention the model card and documentation\")\n",
    "\n",
    "print(f\"\\nüíæ **Files Created:**\")\n",
    "print(f\"   ‚Ä¢ Model: {MODEL_SAVE_PATH}/\")\n",
    "print(f\"   ‚Ä¢ Training curves: /content/drive/MyDrive/training_curves.png\")\n",
    "print(f\"   ‚Ä¢ Model card: {MODEL_SAVE_PATH}/README.md\")\n",
    "print(f\"   ‚Ä¢ Metadata: {MODEL_SAVE_PATH}/metadata.pkl\")\n",
    "\n",
    "print(\"\\nüéä **Congratulations!** Your DistilBERT news classifier is ready for production!\")\n",
    "print(\"Ready to write that Medium post! üìö‚ú®\")\n",
    "\n",
    "# ================================================================================\n",
    "# BONUS: Quick inference test function for demonstration\n",
    "# ================================================================================\n",
    "\n",
    "def demo_classifier_live():\n",
    "    \"\"\"Interactive demo for the classifier\"\"\"\n",
    "    print(\"\\nüéÆ LIVE DEMO - Try Your Own Headlines!\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    demo_articles = [\n",
    "        \"Breaking: Major earthquake hits California coast\",\n",
    "        \"Apple announces new iPhone with revolutionary AI features\",\n",
    "        \"Fed raises interest rates amid inflation concerns\",\n",
    "        \"Local high school wins state basketball championship\",\n",
    "        \"New study shows Mediterranean diet reduces heart disease risk\",\n",
    "        \"Taylor Swift announces world tour dates\",\n",
    "        \"Climate activists protest outside government buildings\"\n",
    "    ]\n",
    "\n",
    "    print(\"Demo headlines:\")\n",
    "    for i, headline in enumerate(demo_articles, 1):\n",
    "        result = classifier.predict(headline, return_probabilities=False)\n",
    "        print(f\"{i}. '{headline}'\")\n",
    "        print(f\"   ‚Üí {result['predicted_category']} ({result['confidence']:.2f})\")\n",
    "\n",
    "    return demo_articles\n",
    "\n",
    "# Run the demo\n",
    "demo_results = demo_classifier_live()\n",
    "\n",
    "print(\"\\nüî• **DATA LEAKAGE FIX APPLIED SUCCESSFULLY!**\")\n",
    "print(\"‚úÖ Model now uses ONLY headline + content for training\")\n",
    "print(\"‚ùå Excluded: ai_summary, reasoning fields\")\n",
    "print(\"üéØ Expected: Much better real-world performance!\")\n",
    "print(\"\\nüöÄ **Run this notebook and your test examples should now classify correctly!**\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "088e26a8967c44589a61876cd1d663f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12561fd94f4842b38db531abc1b35351",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a05a70b345a242e0ae7e3335ccc85f0b",
      "value": "‚Äá971/971‚Äá[00:00&lt;00:00,‚Äá1620.90‚Äáexamples/s]"
     }
    },
    "12561fd94f4842b38db531abc1b35351": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d831053433341d1a7d380190b48cc29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "22e7241eed774218a4ff2de5c351ae18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "283896a95c944ab7adb1f0ff179a88b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3387814592af4606b6fb044666a88dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a953cfbe4b14fab946617f7881d640d",
      "max": 3397,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d831053433341d1a7d380190b48cc29",
      "value": 3397
     }
    },
    "3c0c2551e8de4736bde7397d9669284b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "449c09e57ebe4d9684bdae6e46efd36a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "467377c4c05e4d35b6958c8f4c8dca51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_504b3d0da6794480bf52f59f0ed42c97",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_90997ef0abd248f1a414807bf15e393f",
      "value": "‚Äá486/486‚Äá[00:00&lt;00:00,‚Äá1592.99‚Äáexamples/s]"
     }
    },
    "504b3d0da6794480bf52f59f0ed42c97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5993e3ed21f747d580f82858e862aac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a324ca7f7b1847a3a3fed2f9eef6c437",
      "max": 971,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_449c09e57ebe4d9684bdae6e46efd36a",
      "value": 971
     }
    },
    "60c7bc87e8994d3095e413cfa0cf7927": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ddffb2935c23422a9121435ba08d7c50",
       "IPY_MODEL_3387814592af4606b6fb044666a88dc8",
       "IPY_MODEL_9bea02f9f01c4ada8cb11f7aad4d5d45"
      ],
      "layout": "IPY_MODEL_86d5dca5e1d44561a9d42619a10454d0"
     }
    },
    "66cf79bdc9964952adc330343befa8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a953cfbe4b14fab946617f7881d640d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86d5dca5e1d44561a9d42619a10454d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90997ef0abd248f1a414807bf15e393f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99744c9e7754466bbfac58a402ba9ab8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bea02f9f01c4ada8cb11f7aad4d5d45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f71842d131144b3d8d754598db09bd8a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eb3583b3b28e4dde9b6c56b5ceceaf2b",
      "value": "‚Äá3397/3397‚Äá[00:02&lt;00:00,‚Äá1583.44‚Äáexamples/s]"
     }
    },
    "a05a70b345a242e0ae7e3335ccc85f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a324ca7f7b1847a3a3fed2f9eef6c437": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a42f438e010f44a0af13159142638548": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da49c23a90284a1eb704a169963d8a97",
       "IPY_MODEL_5993e3ed21f747d580f82858e862aac3",
       "IPY_MODEL_088e26a8967c44589a61876cd1d663f2"
      ],
      "layout": "IPY_MODEL_22e7241eed774218a4ff2de5c351ae18"
     }
    },
    "ad8b2e3affb44723bff210f9eafc2e7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c417ecee465243cf9e29f186598726f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_283896a95c944ab7adb1f0ff179a88b8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_66cf79bdc9964952adc330343befa8c2",
      "value": "Map:‚Äá100%"
     }
    },
    "da49c23a90284a1eb704a169963d8a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd87f6b8cbe54e0fbb4d95a02457da1c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_99744c9e7754466bbfac58a402ba9ab8",
      "value": "Map:‚Äá100%"
     }
    },
    "dd87f6b8cbe54e0fbb4d95a02457da1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddffb2935c23422a9121435ba08d7c50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb8517f900244868add4d1abd64219d9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e211ec8f05c24336a09ace6472328f9f",
      "value": "Map:‚Äá100%"
     }
    },
    "e211ec8f05c24336a09ace6472328f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8ae7464ea2f4665966b84059d6b4e27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c417ecee465243cf9e29f186598726f9",
       "IPY_MODEL_fb9619954c6d45a78d267c942275a712",
       "IPY_MODEL_467377c4c05e4d35b6958c8f4c8dca51"
      ],
      "layout": "IPY_MODEL_ad8b2e3affb44723bff210f9eafc2e7d"
     }
    },
    "eb3583b3b28e4dde9b6c56b5ceceaf2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb8517f900244868add4d1abd64219d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5f9bbb6c38c4dafb751282c857fdc23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f71842d131144b3d8d754598db09bd8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb9619954c6d45a78d267c942275a712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5f9bbb6c38c4dafb751282c857fdc23",
      "max": 486,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c0c2551e8de4736bde7397d9669284b",
      "value": 486
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
